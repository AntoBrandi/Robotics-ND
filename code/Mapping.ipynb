{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Map Notebook\n",
    "This notebook contains the functions from the lesson and provides a framework for extracting a map of the area based on the raw images:\n",
    "\n",
    "* Load the recorded images from the Rover Simulator\n",
    "For each frame apply the following:\n",
    "* Perspective transformation of the image using the parameters extracted from the grid images\n",
    "* Color Treshold of the image that filters out only the available areas\n",
    "* Warp from camera-centric to rover-centric coordinates\n",
    "* Rotate and Translate from rover-centric to the world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib qt # Choose %matplotlib qt to plot to an interactive window (note it may show up behind your browser)\n",
    "# Make some of the relevant imports\n",
    "import cv2 # OpenCV for perspective transform\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc # For saving images as needed\n",
    "import glob  # For reading in a list of images from a folder\n",
    "import imageio\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "image_width = 320\n",
    "image_height = 160\n",
    "scale = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color Treshold**\n",
    "\n",
    "Function that takes as its input a color image and a 3-tuple of color threshold values (integer values from 0 to 255), and outputs a single-channel binary image, which is to say an image with a single color channel where every pixel is set to one or zero. In this case, all pixels that were above the threshold should be assigned a value of 1, and those below a value of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_thresh(img, rgb_thresh=(180, 180, 180)):\n",
    "    # Create an array of zeros same xy size as img, but single channel\n",
    "    color_select = np.zeros_like(img[:,:,0])\n",
    "    # Require that each pixel be above all three threshold values in RGB\n",
    "    # above_thresh will now contain a boolean array with \"True\"\n",
    "    # where threshold was met\n",
    "    above_thresh = (img[:,:,0] > rgb_thresh[0]) \\\n",
    "                & (img[:,:,1] > rgb_thresh[1]) \\\n",
    "                & (img[:,:,2] > rgb_thresh[2])\n",
    "    # Index the array of zeros with the boolean array and set to 1\n",
    "    color_select[above_thresh] = 1\n",
    "    # Return the binary image\n",
    "    return color_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perspective Transformation**\n",
    "\n",
    "From the image select the four points of a square in the robot camera view. Then using opencv deform the image to be on a flat surface and each square in the robot camera is mapped to a flat square in the deformed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspect_transform(img):\n",
    "    # Define calibration box in source (actual) and destination (desired) coordinates\n",
    "    # These source and destination points are defined to warp the image\n",
    "    # to a grid where each 10x10 pixel square represents 1 square meter\n",
    "    dst_size = 5 \n",
    "    # Set a bottom offset to account for the fact that the bottom of the image \n",
    "    # is not the position of the rover but a bit in front of it\n",
    "    bottom_offset = 6\n",
    "    src = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "    dst = np.float32([[image_width/2 - dst_size, image_height - bottom_offset],\n",
    "                  [image_width/2 + dst_size, image_height - bottom_offset],\n",
    "                  [image_width/2 + dst_size, image_height - 2*dst_size - bottom_offset], \n",
    "                  [image_width/2 - dst_size, image_height - 2*dst_size - bottom_offset],\n",
    "                  ])\n",
    "\n",
    "    # Get transform matrix using cv2.getPerspectivTransform()\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # Warp image using cv2.warpPerspective()\n",
    "    # keep same size as input image\n",
    "    warped = cv2.warpPerspective(img, M, (image_width, image_height))\n",
    "    # Return the result\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rover Centric Coordinates**\n",
    "\n",
    "Converts an image from Camera-Centric Coordinates to Rover-Centric Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rover_coords(binary_img):\n",
    "    # this function calculate pixel positions with reference to the rover \n",
    "    # position being at the center bottom of the image.  \n",
    "    ypos, xpos = binary_img.nonzero()\n",
    "    x_pixel = -(ypos - image_height).astype(np.float)\n",
    "    y_pixel = -(xpos - image_width/2 ).astype(np.float)\n",
    "    return x_pixel, y_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rover Centric Coordinates to World**\n",
    "\n",
    "Application of a rotation matrix composed by a rotation and a translation to the rover-centric map\n",
    "in order to obtain the map in the world reference frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply a rotation to pixel positions\n",
    "def rotate_pix(xpix, ypix, yaw):\n",
    "    # Convert yaw to radians\n",
    "    # Apply a rotation\n",
    "    # yaw angle is recorded in degrees so first convert to radians\n",
    "    yaw_rad = yaw * np.pi / 180\n",
    "    xpix_rotated = xpix * np.cos(yaw_rad) - ypix * np.sin(yaw_rad)\n",
    "    ypix_rotated = xpix * np.sin(yaw_rad) + ypix * np.cos(yaw_rad)\n",
    "    # Return the result  \n",
    "    return xpix_rotated, ypix_rotated\n",
    "\n",
    "# Define a function to perform a translation\n",
    "def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n",
    "    # Apply a scaling and a translation\n",
    "    # Assume a scale factor of 10 between world space pixels and rover space pixels\n",
    "    # Perform translation and convert to integer since pixel values can't be float\n",
    "    xpix_translated = np.int_(xpos + (xpix_rot / scale))\n",
    "    ypix_translated = np.int_(ypos + (ypix_rot / scale))\n",
    "    # Return the result  \n",
    "    return xpix_translated, ypix_translated\n",
    "\n",
    "# Define a function to apply rotation and translation (and clipping)\n",
    "# Once you define the two functions above this function should work\n",
    "def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "    # Apply rotation\n",
    "    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "    # Apply translation\n",
    "    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "    # Clip to world_size\n",
    "    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n",
    "    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n",
    "    # Return the result\n",
    "    return x_pix_world, y_pix_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in saved data and ground truth map of the world\n",
    "The next cell is all setup to read your saved data into a `pandas` dataframe.  Here you'll also read in a \"ground truth\" map of the world, where white pixels (pixel value = 1) represent navigable terrain.  \n",
    "\n",
    "After that, we'll define a class to store telemetry data and pathnames to images.  When you instantiate this class (`data = Databucket()`) you'll have a global variable called `data` that you can refer to for telemetry and map data within the `process_image()` function in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file as a dataframe\n",
    "\n",
    "df = pd.read_csv('../Recordings/robot_log.csv', delimiter=';', decimal='.')\n",
    "csv_img_list = df[\"Path\"].tolist() # Create list of image pathnames\n",
    "# Read in ground truth map and create a 3-channel image with it\n",
    "ground_truth = mpimg.imread('../Recordings/calibration_images/map_bw.png')\n",
    "ground_truth_3d = np.dstack((ground_truth*0, ground_truth*255, ground_truth*0)).astype(np.float)\n",
    "\n",
    "# Creating a class to be the data container\n",
    "# Will read in saved data from csv file and populate this object\n",
    "# Worldmap is instantiated as 200 x 200 grids corresponding \n",
    "# to a 200m x 200m space (same size as the ground truth map: 200 x 200 pixels)\n",
    "# This encompasses the full range of output position values in x and y from the sim\n",
    "class Databucket():\n",
    "    def __init__(self):\n",
    "        self.images = csv_img_list  \n",
    "        self.xpos = df[\"X_Position\"].values\n",
    "        self.ypos = df[\"Y_Position\"].values\n",
    "        self.yaw = df[\"Yaw\"].values\n",
    "        self.count = 0 # This will be a running index\n",
    "        self.worldmap = np.zeros((200, 200, 3)).astype(np.float)\n",
    "        self.ground_truth = ground_truth_3d # Ground truth worldmap\n",
    "\n",
    "# Instantiate a Databucket().. this will be a global variable/object\n",
    "# that you can refer to in the process_image() function below\n",
    "data = Databucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to processes stored images\n",
    "\n",
    "The `process_image()` function below adds perception step processes (functions defined above) to perform image analysis and mapping.  The following cell is all set up to use this `process_image()` function in conjunction with the `moviepy` video processing package to create a video from the images you saved taking data in the simulator.  \n",
    "\n",
    "The function `process_image()` will be passed individual images into and outputs an image called `output_image` that will be stored as one frame of video.  You can make a mosaic of the various steps of your analysis process and add text as you like (example provided below).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pass stored images to\n",
    "# reading rover position and yaw angle from csv file\n",
    "# This function will be used by moviepy to create an output video\n",
    "def process_image(img):\n",
    "    # Use the Databucket() object defined above\n",
    "    # to print the current x, y and yaw values \n",
    "    rover_xpos = data.xpos[data.count]\n",
    "    rover_ypos = data.ypos[data.count]\n",
    "    rover_yaw = data.yaw[data.count]\n",
    "\n",
    "    # 1) Apply perspective transform\n",
    "    warped = perspect_transform(img)\n",
    "    \n",
    "    # 2) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "    colored = color_thresh(warped)\n",
    "    \n",
    "    # 3) Convert thresholded image pixel values to rover-centric coords\n",
    "    xpix, ypix = rover_coords(colored)\n",
    "    \n",
    "    # 4) Convert rover-centric pixel values to world coords\n",
    "    x_world, y_world = pix_to_world(xpix, ypix, rover_xpos, \n",
    "                                rover_ypos, rover_yaw, \n",
    "                                data.worldmap.shape[0], scale)\n",
    "    \n",
    "    # 5) Update worldmap (to be displayed on right side of screen)\n",
    "        # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "        #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "        #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "    data.worldmap[y_world, x_world, 0] = 255\n",
    "\n",
    "    # 6) Make a mosaic image, below is some example code\n",
    "        # First create a blank image (can be whatever shape you like)\n",
    "    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n",
    "        # Next you can populate regions of the image with various output\n",
    "        # Here I'm putting the original image in the upper left hand corner\n",
    "    output_image[0:img.shape[0], 0:img.shape[1]] = img\n",
    "    \n",
    "        # Add the warped image in the upper right hand corner\n",
    "    output_image[0:img.shape[0], img.shape[1]:] = warped\n",
    "    \n",
    "        # Add the constructed map in the lower right hand corner\n",
    "    output_image[img.shape[0]:, img.shape[1]:img.shape[1]+data.worldmap.shape[1]] = data.worldmap\n",
    "\n",
    "        # Overlay worldmap with ground truth map\n",
    "    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n",
    "        # Flip map overlay so y-axis points upward and add to output_image \n",
    "    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n",
    "\n",
    "\n",
    "        # Then putting some text over the image\n",
    "    cv2.putText(output_image,\"Populate this image with your analyses to make a video!\", (20, 20), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    if data.count < len(data.images) - 1:\n",
    "        data.count += 1 # Keep track of the index in the Databucket()\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make and Play a video from processed image data\n",
    "Use the [moviepy](https://zulko.github.io/moviepy/) library to process images and create a video.\n",
    "\n",
    "After that, use an inline video player to play the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 2/1561 [00:00<01:41, 15.37it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output/test_mapping.mp4.\n",
      "Moviepy - Writing video output/test_mapping.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output/test_mapping.mp4\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "# Define pathname to save the output video\n",
    "output = 'output/test_mapping.mp4'\n",
    "data = Databucket() # Re-initialize data in case you're running this cell multiple times\n",
    "clip = ImageSequenceClip(data.images, fps=60) # Note: output video will be sped up because \n",
    "                                          # recording rate in simulator is fps=25\n",
    "new_clip = clip.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time new_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"output/test_mapping.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
